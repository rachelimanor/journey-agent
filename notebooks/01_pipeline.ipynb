{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8647ef3c-e0e6-4884-9a32-47fabd4cfce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import openai, pandas as pd, requests, bs4, readability, lxml, praw, os\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96340db7-8cdb-479e-934c-4f2ffff74091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI: True\n",
      "SERPAPI: True\n",
      "REDDIT: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv; load_dotenv()\n",
    "import os\n",
    "print(\"OPENAI:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "print(\"SERPAPI:\", bool(os.getenv(\"SERPAPI_API_KEY\")))\n",
    "print(\"REDDIT:\", all([os.getenv(\"REDDIT_CLIENT_ID\"), os.getenv(\"REDDIT_CLIENT_SECRET\"), os.getenv(\"REDDIT_USER_AGENT\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2413e52c-eff9-4aec-9c47-ce5b15f0c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 20251022_1918\n"
     ]
    }
   ],
   "source": [
    "# Configuation\n",
    "\n",
    "# --- config ---\n",
    "TOPIC = \"Private-party used-car buying in SF Bay Area\"\n",
    "SUBREDDITS = [\"UsedCars\"] #, \"MechanicAdvice\", \"WhatCarShouldIBuy\", \"cars\", \"Scams\", \"AskCarsales\"]\n",
    "QUERY_STRINGS = [\n",
    "    'Buying a used car']\n",
    "    #'private party payment cashier check',\n",
    "    #'private party title lien transfer',\n",
    "    #'private party escrow payment',\n",
    "    #'inspection OBD checklist private sale',\n",
    "    #'wire vs cash private sale',\n",
    "#]\n",
    "\n",
    "\n",
    "SINCE_DAYS = 730        # ~24 months * 365\n",
    "THREADS_PER_QUERY = 4   # keep small for first run\n",
    "TOP_COMMENTS = 4\n",
    "\n",
    "import os, json, time, math, datetime as dt, re, pathlib # re = pyton regex. pathlib = an easy way to manage env vars.\n",
    "import praw as praw # Reddit API\n",
    "\n",
    "\n",
    "# loads the environment vars from .env\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "\n",
    "# Creates an env var for the outputs, and creates a data/exports dir that corresponds with the env var\n",
    "EXPORTS = pathlib.Path(\"../data/exports\"); EXPORTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "RUN_ID = dt.datetime.utcnow().strftime(\"%Y%m%d_%H%M\")\n",
    "print(\"Run:\", RUN_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1955481-706a-461d-8751-54d41c5ec1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threads found (after recency + dedupe): 4\n",
      "- r/UsedCars | 2025-08-01T20:49:40Z | Is buying a used car still the way to go?…\n",
      "- r/UsedCars | 2025-10-11T17:56:53Z | how do you buy a used car?…\n",
      "- r/UsedCars | 2025-08-07T16:51:57Z | How do people even buy used cars.…\n",
      "- r/UsedCars | 2025-07-25T15:23:52Z | I sold a classic car to this guy (private sale) and now he's saying I misrepresented the car and he's giving me the opportunity to buy the car back.…\n"
     ]
    }
   ],
   "source": [
    "# --- Stage 1: Researcher (Reddit via PRAW search) ---\n",
    "\n",
    "# connect to Reddit (uses your .env values)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"REDDIT_USER_AGENT\"),\n",
    ")\n",
    "\n",
    "reddit.read_only = True\n",
    "\n",
    "# simple call: fetch one hot post from r/UsedCars\n",
    "#sub = reddit.subreddit(\"UsedCars\")\n",
    "#post = next(sub.hot(limit=1))\n",
    "#print(\"OK:\", post.title[:80])\n",
    "\n",
    "\n",
    "def search_threads(query, subreddits, limit_per_sub, since_days):\n",
    "    \"\"\" Search Reddit for `query` across `subreddits`, keeping posts newer than `since_days`.\n",
    "\n",
    "    Args:\n",
    "        query: Search string (e.g., \"private party escrow\").\n",
    "        subreddits: List like [\"UsedCars\", \"MechanicAdvice\"].\n",
    "        limit_per_sub: Max results per subreddit (before filtering).\n",
    "        since_days: Recency window; older posts are dropped.\n",
    "\n",
    "    Returns:\n",
    "        A de-duplicated list of thread dicts with id, title, permalink, timestamps, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    after_ts = time.time() - since_days * 24 * 3600\n",
    "    found = []\n",
    "    \n",
    "    for sub in subreddits:\n",
    "        sr = reddit.subreddit(sub)\n",
    "        # Reddit search (relevance) with a per-subreddit cap\n",
    "        for submission in sr.search(query, sort=\"relevance\", limit=limit_per_sub):\n",
    "            if submission.created_utc < after_ts:\n",
    "                continue\n",
    "            found.append({\n",
    "                \"id\": submission.id,\n",
    "                \"subreddit\": str(sub),\n",
    "                \"title\": submission.title,\n",
    "                \"permalink\": \"https://www.reddit.com\" + submission.permalink,\n",
    "                \"created_utc\": submission.created_utc,\n",
    "                \"created_iso\": dt.datetime.utcfromtimestamp(submission.created_utc).isoformat() + \"Z\",\n",
    "                \"is_self\": submission.is_self,\n",
    "                \"url\": submission.url,\n",
    "                \"query\": query,\n",
    "            })\n",
    "    # de-dupe by submission id\n",
    "    seen = set()\n",
    "    dedup = []\n",
    "    for r in found:\n",
    "        if r[\"id\"] in seen:\n",
    "            continue\n",
    "        seen.add(r[\"id\"])\n",
    "        dedup.append(r)\n",
    "    return dedup\n",
    "\n",
    "\n",
    "# run searches over your configured queries/subreddits\n",
    "all_threads = []\n",
    "for q in QUERY_STRINGS:\n",
    "    all_threads += search_threads(\n",
    "        query=q, \n",
    "        subreddits=SUBREDDITS, \n",
    "        limit_per_sub=THREADS_PER_QUERY, \n",
    "        since_days=SINCE_DAYS\n",
    "    )\n",
    "\n",
    "# global de-dup (across queries)\n",
    "seen_ids, threads = set(), []\n",
    "for r in all_threads:\n",
    "    if r[\"id\"] in seen_ids: \n",
    "        continue\n",
    "    seen_ids.add(r[\"id\"]); threads.append(r)\n",
    "\n",
    "print(f\"Threads found (after recency + dedupe): {len(threads)}\")\n",
    "# quick peek\n",
    "for t in threads[:5]:\n",
    "    print(f\"- r/{t['subreddit']} | {t['created_iso']} | {t['title'][:200]}…\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cab9c-2cd2-4116-bb93-0adb1fe0d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 2: Crawler — expand each thread with post + top comments ---\n",
    "\n",
    "def fetch_thread(submission_id: str, top_n: int = TOP_COMMENTS) -> dict:\n",
    "    sub = reddit.submission(id=submission_id)\n",
    "    sub.comment_sort = \"top\"\n",
    "    # flatten \"More comments…\" so we get real comments\n",
    "    sub.comments.replace_more(limit=0)\n",
    "\n",
    "    comments = []\n",
    "    for c in sub.comments[:top_n]:\n",
    "        comments.append({\n",
    "            \"author\": str(c.author) if c.author else \"deleted\",\n",
    "            \"body\": c.body,\n",
    "            \"permalink\": f\"https://www.reddit.com{c.permalink}\",\n",
    "            \"created_utc\": c.created_utc,\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"thread_id\": sub.id,\n",
    "        \"subreddit\": str(sub.subreddit),\n",
    "        \"thread_title\": sub.title,\n",
    "        \"selftext\": sub.selftext or \"\",\n",
    "        \"permalink\": f\"https://www.reddit.com{sub.permalink}\",\n",
    "        \"created_utc\": sub.created_utc,\n",
    "        \"comments\": comments,\n",
    "    }\n",
    "\n",
    "# run crawler over the threads we just found\n",
    "bundles = []\n",
    "for t in threads:\n",
    "    try:\n",
    "        bundles.append(fetch_thread(t[\"id\"], top_n=TOP_COMMENTS))\n",
    "    except Exception as e:\n",
    "        print(\"crawl error:\", t[\"id\"], e)\n",
    "\n",
    "print(f\"Bundles: {len(bundles)} (each has post + up to {TOP_COMMENTS} top comments)\")\n",
    "# quick peek\n",
    "if bundles:\n",
    "    b = bundles[0]\n",
    "    print(\"EXAMPLE:\", b[\"thread_title\"][:90], \"| comments:\", len(b[\"comments\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40a1a5-9103-4d01-a781-75820ad2aa3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (journey)",
   "language": "python",
   "name": "journey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
